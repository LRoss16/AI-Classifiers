{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import Image\n",
    "from keras.layers import SimpleRNN, Embedding, Dense, LSTM\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import seaborn as sns; sns.set()\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set data to only use the 6 categories chosen\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med', 'rec.sport.hockey', 'rec.motorcycles', 'sci.space']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset = 'train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4048"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)\n",
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sandvik@newton.apple.com (Kent Sandvik)\n",
      "Subject: Re: Genocide is Caused by Atheism\n",
      "Organization: Cookamunga Tourist Bureau\n",
      "Lines: 26\n",
      "\n",
      "In article <1993Apr19.113255.27550@monu6.cc.monash.edu.au>,\n",
      "darice@yoyo.cc.monash.edu.au (Fred Rice) wrote:\n",
      "> >Fred, the problem with such reasoning is that for us non-believers\n",
      "> >we need a better measurement tool to state that person A is a\n",
      "> >real Muslim/Christian, while person B is not. As I know there are\n",
      "> >no such tools, and anyone could believe in a religion, misuse its\n",
      "> >power and otherwise make bad PR. It clearly shows the sore points\n",
      "> >with religion -- in other words show me a movement that can't spin\n",
      "> >off Khomeinis, Stalins, Davidians, Husseins... *).\n",
      "> \n",
      "> I don't think such a system exists.  I think the reason for that is an\n",
      "> condition known as \"free will\".  We humans have got it.  Anybody, using\n",
      "> their free-will, can tell lies and half-truths about *any* system and\n",
      "> thus abuse it for their own ends.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[1].split(\"\\n\")[:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 4, 3, 1, 0, 2, 4, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n",
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'rec.sport.hockey', 'sci.med', 'sci.space', 'soc.religion.christian']\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4048, 51970)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use count vectorizer method to transform data to vectors\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4048, 51970)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get words most frequently used\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transform = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = transform.transform(X_train_counts)\n",
    "X_train_tf.shape\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In space no one will hear you scream' => sci.space\n",
      "'Computer needs more RAM' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Test classifier to see if it can correctly predic where something should go \n",
    "classifier = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "test = ['In space no one will hear you scream', 'Computer needs more RAM']\n",
    "counts = count_vect.transform(test)\n",
    "tfidf = tfidf_transformer.transform(counts)\n",
    "\n",
    "prediction = classifier.predict(tfidf)\n",
    "\n",
    "for doc, category in zip(test, prediction):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use pipeline method to create the classifier work flow\n",
    "textClassifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "textClassifier.fit(twenty_train.data, twenty_train.target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8730040846639435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test and evaluate\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = textClassifier.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9279613813590791"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For SVM the pipeline just needs to be adjusted\n",
    "textClassifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "textClassifier.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "predicted = textClassifier.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.94      0.77      0.85       319\n",
      "         comp.graphics       0.90      0.95      0.93       389\n",
      "       rec.motorcycles       0.96      0.99      0.98       398\n",
      "      rec.sport.hockey       0.97      1.00      0.98       399\n",
      "               sci.med       0.95      0.85      0.90       396\n",
      "             sci.space       0.94      0.94      0.94       394\n",
      "soc.religion.christian       0.85      0.95      0.90       398\n",
      "\n",
      "              accuracy                           0.93      2693\n",
      "             macro avg       0.93      0.92      0.92      2693\n",
      "          weighted avg       0.93      0.93      0.93      2693\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>sci.med</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>soc.religion.christian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>371</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>396</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>398</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>336</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alt.atheism  comp.graphics  rec.motorcycles  rec.sport.hockey  sci.med  \\\n",
       "0          246              4                3                 3       10   \n",
       "1            3            371                1                 1        1   \n",
       "2            0              1              396                 1        0   \n",
       "3            0              1                0               398        0   \n",
       "4            6             19               10                 7      336   \n",
       "5            0             13                1                 1        5   \n",
       "6            7              3                1                 1        1   \n",
       "\n",
       "   sci.space  soc.religion.christian  \n",
       "0          4                      49  \n",
       "1          8                       4  \n",
       "2          0                       0  \n",
       "3          0                       0  \n",
       "4          6                      12  \n",
       "5        372                       2  \n",
       "6          5                     380  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use metrics method to get report on how classifier did\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "                                    \n",
    "confusion = metrics.confusion_matrix(twenty_test.target, predicted)\n",
    "pd.DataFrame(data = confusion, columns = twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[246,   4,   3,   3,  10,   4,  49],\n",
       "       [  3, 371,   1,   1,   1,   8,   4],\n",
       "       [  0,   1, 396,   1,   0,   0,   0],\n",
       "       [  0,   1,   0, 398,   0,   0,   0],\n",
       "       [  6,  19,  10,   7, 336,   6,  12],\n",
       "       [  0,  13,   1,   1,   5, 372,   2],\n",
       "       [  7,   3,   1,   1,   1,   5, 380]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "LABEL: rec.autos = 7\n",
      "Test\n",
      "From: adamsj@gtewd.mtv.gtegsc.com\n",
      "Subject: Re: Homosexuality issues in Christianity\n",
      "Reply-To: adamsj@gtewd.mtv.gtegsc.com\n",
      "Organization: GTE Govt. Systems, Electronics Def. Div.\n",
      "Lines: 18\n",
      "\n",
      "In article <May.13.02.29.39.1993.1505@geneva.rutgers.edu>, revdak@netcom.com (D. Andrew Kille) writes:\n",
      "> Of course the whole issue is one of discernment.  It may be that Satan\n",
      "> is trying to convince us that we know more than God.  Or it may be that\n",
      "> God is trying (as God did with Peter) to teach us something we don't\n",
      "> know- that \"God shows no partiality, but in every nation anyone who fears\n",
      "> him and does what is right is acceptable to him.\" (Acts 10:34-35).\n",
      "> \n",
      "> revdak@netcom.com\n",
      "\n",
      "Fine, but one of the points of this entire discussion is that \"we\"\n",
      "(conservative, reformed christians - this could start an argument...\n",
      "But isn't this idea that homosexuality is ok fairly \"new\" [this\n",
      "century] ? Is there any support for this being a viable viewpoint\n",
      "before this century? I don't know.) don't believe that homosexuality\n",
      "is \"acceptable to Him\". So your scripture quotation doesn't work for\n",
      "\"us\".\n",
      "\n",
      "-jeff adams-\n",
      "\n",
      "LABEL: soc.religion.christian = 15\n"
     ]
    }
   ],
   "source": [
    "train = fetch_20newsgroups()\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "print(\"Train\")\n",
    "print(train.data[0].strip())\n",
    "print(\"\\nLABEL:\", train.target_names[train.target[0]],\n",
    "      \"=\", train.target[0])\n",
    "print(\"Test\")\n",
    "print(test.data[-1].strip())\n",
    "print(\"\\nLABEL:\", test.target_names[test.target[-1]],\n",
    "      \"=\", test.target[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lewis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenizer(text):\n",
    "       return text.split()\n",
    "\n",
    "def tokenizer_stemmer(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer(text)]#text.split()]\n",
    "\n",
    "\n",
    "def stop_removal(text):\n",
    "       return [w for w in text if not w in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use method so network knows that abbreviations such as don't should be do not\n",
    "\n",
    "import re\n",
    "\n",
    "vocabulary = (\n",
    "    (re.compile(r\"\\bdon't\\b\"), \"do not\"),\n",
    "    (re.compile(r\"\\bit's\\b\"), \"it is\"),\n",
    "    (re.compile(r\"\\bi'm\\b\"), \"i am\"),\n",
    "    (re.compile(r\"\\bi've\\b\"), \"i have\"),\n",
    "    (re.compile(r\"\\bcan't\\b\"), \"cannot\"),\n",
    "    (re.compile(r\"\\bdoesn't\\b\"), \"does not\"),\n",
    "    (re.compile(r\"\\bthat's\\b\"), \"that is\"),\n",
    "    (re.compile(r\"\\bdidn't\\b\"), \"did not\"),\n",
    "    (re.compile(r\"\\bi'd\\b\"), \"i would\"),\n",
    "    (re.compile(r\"\\byou're\\b\"), \"you are\"),\n",
    "    (re.compile(r\"\\bisn't\\b\"), \"is not\"),\n",
    "    (re.compile(r\"\\bi'll\\b\"), \"i will\"),\n",
    "    (re.compile(r\"\\bthere's\\b\"), \"there is\"),\n",
    "    (re.compile(r\"\\bwon't\\b\"), \"will not\"),\n",
    "    (re.compile(r\"\\bwoudn't\\b\"), \"would not\"),\n",
    "    (re.compile(r\"\\bhe's\\b\"), \"he is\"),\n",
    "    (re.compile(r\"\\bthey're\\b\"), \"they are\"),\n",
    "    (re.compile(r\"\\bwe're\\b\"), \"we are\"),\n",
    "    (re.compile(r\"\\blet's\\b\"), \"let us\"),\n",
    "    (re.compile(r\"\\bhaven't\\b\"), \"have not\"),\n",
    "    (re.compile(r\"\\bwhat's\\b\"), \"what is\"),\n",
    "    (re.compile(r\"\\baren't\\b\"), \"are not\"),\n",
    "    (re.compile(r\"\\bwasn't\\b\"), \"was not\"),\n",
    "    (re.compile(r\"\\bwouldn't\\b\"), \"would not\"),\n",
    ")\n",
    "\n",
    "def fix_apostrophes(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    for pattern, replacement in vocabulary:\n",
    "        text = pattern.sub(replacement, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "text_train = list(map(fix_apostrophes, train.data))\n",
    "text_test = list(map(fix_apostrophes, test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit what is used to what was found in training set\n",
    "Unique_Words= 10000\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=Unique_Words,\n",
    "    lower=False,\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126595 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "# the 10000 most frequent training words are used\n",
    "\n",
    "# generate index vectors from both train and test\n",
    "\n",
    "seq_train = tokenizer.texts_to_sequences(text_train)\n",
    "seq_test = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 11314\n",
      "Training data shape: (11314, 1000)\n",
      "Training label shape: (11314, 20)\n",
      "\n",
      "Test set size: 7532\n",
      "Test data shape: (7532, 1000)\n",
      "Test label shape: (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Use pad sequences method so only equally sized vectors are used \n",
    "Seq_length= 1000\n",
    "\n",
    "x_train = pad_sequences(seq_train, maxlen=Seq_length, truncating='post')\n",
    "x_test = pad_sequences(seq_test, maxlen=Seq_length, truncating='post')\n",
    "\n",
    "y_train = to_categorical(np.asarray(train.target))\n",
    "y_test = to_categorical(np.asarray(test.target))\n",
    "print('Training set size:', len(train.data))\n",
    "print('Training data shape:', x_train.shape)\n",
    "print('Training label shape:', y_train.shape)\n",
    "print('\\nTest set size:', len(test.data))\n",
    "print('Test data shape:', x_test.shape)\n",
    "print('Test label shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                660       \n",
      "=================================================================\n",
      "Total params: 322,740\n",
      "Trainable params: 322,740\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build network\n",
    "\n",
    "embedding_size=32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000,\n",
    " output_dim=embedding_size,\n",
    " input_length=1000))\n",
    "model.add(SimpleRNN(units=embedding_size))\n",
    "\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    " metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lewis\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/10\n",
      "9051/9051 [==============================] - 107s 12ms/step - loss: 0.2765 - acc: 0.9244 - val_loss: 0.1983 - val_acc: 0.9500\n",
      "Epoch 2/10\n",
      "9051/9051 [==============================] - 112s 12ms/step - loss: 0.1968 - acc: 0.9500 - val_loss: 0.1958 - val_acc: 0.9500\n",
      "Epoch 3/10\n",
      "9051/9051 [==============================] - 160s 18ms/step - loss: 0.1902 - acc: 0.9500 - val_loss: 0.1888 - val_acc: 0.9500\n",
      "Epoch 4/10\n",
      "9051/9051 [==============================] - 157s 17ms/step - loss: 0.1777 - acc: 0.9504 - val_loss: 0.1811 - val_acc: 0.9511\n",
      "Epoch 5/10\n",
      "9051/9051 [==============================] - 120s 13ms/step - loss: 0.1633 - acc: 0.9526 - val_loss: 0.1762 - val_acc: 0.9530\n",
      "Epoch 6/10\n",
      "9051/9051 [==============================] - 105s 12ms/step - loss: 0.1490 - acc: 0.9545 - val_loss: 0.1742 - val_acc: 0.9549\n",
      "Epoch 7/10\n",
      "9051/9051 [==============================] - 106s 12ms/step - loss: 0.1354 - acc: 0.9568 - val_loss: 0.1726 - val_acc: 0.9561\n",
      "Epoch 8/10\n",
      "9051/9051 [==============================] - 110s 12ms/step - loss: 0.1230 - acc: 0.9591 - val_loss: 0.1729 - val_acc: 0.9568\n",
      "Epoch 9/10\n",
      "9051/9051 [==============================] - 114s 13ms/step - loss: 0.1121 - acc: 0.9616 - val_loss: 0.1745 - val_acc: 0.9568\n",
      "Epoch 10/10\n",
      "9051/9051 [==============================] - 107s 12ms/step - loss: 0.1024 - acc: 0.9643 - val_loss: 0.1786 - val_acc: 0.9568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x249a60fd908>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train network\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=60, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532/7532 [==============================] - 32s 4ms/step\n",
      "Test loss is 0.19 accuracy is 0.95 \n"
     ]
    }
   ],
   "source": [
    "acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss is {0:.2f} accuracy is {1:.2f} \".format(acc[0],acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
